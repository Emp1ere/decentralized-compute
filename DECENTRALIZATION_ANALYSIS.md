# Анализ децентрализации системы (текущее состояние)

## Краткий вывод

**Децентрализация на уровне данных сохранена:** два равноправных узла, общая цепочка, синхронизация и консенсус «longest valid chain» работают.  
**Децентрализация на уровне создания блоков не реализована:** почти все блоки создаёт один узел (node-1), потому что весь трафик воркеров и критичных API направлен на него.

---

## Что сохранено (децентрализовано)

| Компонент | Статус | Описание |
|-----------|--------|----------|
| **Два узла** | ✅ | `orchestrator_node_1` и `orchestrator_node_2`, равноправны по коду и конфигу. |
| **Единая цепочка** | ✅ | Оба узла хранят одну и ту же цепочку блоков (после синхронизации). |
| **Синхронизация цепочки** | ✅ | `sync_chain_from_peer()` по таймеру (SYNC_INTERVAL), `replace_chain_from_peer()` по правилу «longest valid chain». |
| **Приём блоков от пира** | ✅ | `POST /receive_block` → `add_block_from_peer()`; блок, созданный на одном узле, рассылается пиру и принимается. |
| **Разрешение форков** | ✅ | При одинаковой длине цепочки выбор по `timestamp`, затем по `hash` последнего блока. |
| **Синхронизация pending** | ✅ | Перед созданием блока вызывается `sync_pending_from_peer()` — объединение pending с пиром. |
| **Ротация лидера (код)** | ✅ | `get_current_leader()` по хешу последнего блока реализован и используется в `/metrics` (информационно). |

Итог: данные (цепочка, балансы) согласованы между узлами; второй узел не «отстаёт» и может принять любой блок от первого.

---

## Что централизовано

| Компонент | Статус | Причина |
|-----------|--------|---------|
| **Создание блоков** | ❌ Фактически один узел | Блок создаёт тот узел, который **принял** `submit_work`. Кто принял — тот и создал блок. |
| **Направление трафика** | ❌ Всё на node-1 | Nginx (`node1_only`) направляет на **orchestrator_node_1**: `/submit_work`, `/get_task`, `/run_worker`, `/me`, `/get_balance`, `/auth/`, `/register`, `/worker_progress`. |
| **Воркеры из дашборда** | ❌ Все идут на node-1 | `ORCHESTRATOR_URL_FOR_WORKER=http://orchestrator_node_1:5000` в docker-compose для обоих узлов. |
| **Использование лидера в submit_work** | ❌ Не используется | `get_current_leader()` в `submit_work` не вызывается; транзакции не перенаправляются лидеру. |

Следствие: все запросы задач и сдача работ идут на node-1 → node-1 создаёт почти все блоки. Node-2 получает их только через `receive_block` и синхронизацию цепочки.

---

## Откуда это взялось

В коде и конфигах заложено явное решение «всё важное вести на node_1», чтобы:

- воркер и проверка баланса видели одни и те же данные (нет расхождений между узлами для одного пользователя);
- награды и баланс были предсказуемы при работе через дашборд и «Запустить воркер».

То есть текущая схема — осознанный компромисс: **консистентность и простота для пользователя в обмен на централизацию создания блоков на одном узле**.

---

## Рекомендации, если нужно усилить децентрализацию

1. **Направлять submit_work на текущего лидера**  
   В `submit_work`: если «мы не лидер», не создавать блок самим, а отправить транзакции лидеру (например, через внутренний вызов или `add_pending_tx` на URL лидера) и дождаться блока от него. Лидер по-прежнему определяется через `get_current_leader()` (по хешу последнего блока). Тогда создание блоков будет чередоваться между узлами.

2. **Распределить трафик через nginx**  
   Часть запросов (например, `submit_work`, `get_task`) отдавать в общий `upstream orchestrator` (ip_hash по двум узлам), а не в `node1_only`. Тогда часть блоков будет создавать node-2. Важно: нужно сохранить консистентность баланса (например, синхронизация цепочки уже это обеспечивает).

3. **Развести воркеров по узлам**  
   Часть воркеров направлять на node-1, часть на node-2 (разные `ORCHESTRATOR_URL` / профили), чтобы нагрузка и создание блоков распределялись.

4. **Метрика распределения блоков**  
   Добавить в блок поле `creator_node_id` (или аналог) и в `/metrics` считать, сколько блоков создал каждый узел, чтобы можно было проверять эффект от изменений.

---

## Сложности при децентрализации создания блоков

Если включить децентрализованное создание блоков (трафик на оба узла и/или перенаправление submit на лидера), появятся следующие сложности.

### Для пользователя

| Сложность | Описание |
|-----------|----------|
| **Задержка в обновлении баланса** | Пользователь сдаёт работу на node-1, блок создаётся на node-2 (лидер). Пока цепочка не синхронизировалась, запрос `get_balance` на node-1 вернёт старый баланс. Нужно либо ждать синхронизации (несколько секунд), либо запрашивать баланс с того же узла, что принял submit (или показывать «обновляется» в UI). |
| **Редирект / другой узел** | Если submit перенаправлять на лидера: воркер получил задачу с node-1, а submit уходит на node-2. Ответ приходит от node-2; при падении node-2 нужен retry на другой узел. Пользователь может не понимать, «на каком узле» он залогинен — лучше считать, что баланс и цепочка в итоге едины. |
| **Таймауты и повторные отправки** | Лидер может быть перегружен или недоступен. Тогда submit должен либо ждать с большим таймаутом, либо возвращать 503 и предлагать повторить. Воркеру нужна логика retry при 5xx. |
| **«Награда не пришла»** | В случае форка блок с наградой пользователя может оказаться в проигравшей ветке (см. ниже). Тогда награда не попадёт в принятую цепочку; пользователь увидит, что submit вернул 200, но баланс не изменился. Нужна явная обработка: повторная отправка или сообщение «подождите, идёт синхронизация». |

### В системе в целом

| Сложность | Описание |
|-----------|----------|
| **Форки и проигравшие блоки** | Два узла одновременно создают блок (разные наборы pending) → две ветки одной длины. Консенсус выбирает одну (по timestamp/hash). Транзакции из **проигравшего** блока в текущей реализации **теряются**: при `replace_chain_from_peer()` локальный `pending_transactions` обнуляется, в новую цепочку не попадают tx, которые были только в отброшенном блоке. |
| **Потеря транзакций при замене цепочки** | В `blockchain.replace_chain_from_peer()` после принятия более длинной цепочки пира выполняется `self.pending_transactions = []`. Все локальные pending, не вошедшие в выигравшую цепочку, отбрасываются. Пользователи, чьи reward/work_receipt были только в проигравшем блоке, не получают награду без повторной отправки. |
| **Гонка двух submit от одного клиента** | Если оба узла принимают submit от одного клиента (две разные задачи), в pending на разных узлах окажутся по паре reward+work_receipt. При синхронизации pending и майнинге возможны два блока с двумя наградами одному клиенту (корректно), но при форке одна из наград может «пропасть» вместе с проигравшим блоком. |
| **Согласованность лидера** | Лидер считается по хешу последнего блока (`get_current_leader()`). Оба узла после синхронизации дают одного и того же лидера. До синхронизации узлы могут по-разному видеть «последний блок» и, значит, разного лидера — тогда часть запросов может уйти не туда. Обычно это смягчают, делая синхронизацию частой или отправляя submit только после проверки актуальной цепочки. |
| **Рост количества форков** | Чем равномернее трафик по узлам, тем чаще возможны одновременные создания блоков и форки. Нужен мониторинг (например, метрика «orphaned_blocks» или «replaced_chain_count») и, при желании, возврат не попавших в цепочку tx в pending (сложнее в реализации). |

### Что нужно доработать при переходе

1. **Не терять tx при замене цепочки** — перед `pending_transactions = []` вычислить множество tx, уже входящих в новую цепочку; оставшиеся локальные pending либо вернуть в `pending_transactions`, либо явно отклонить и вернуть клиенту код/сообщение «повторите submit».  
2. **Понятное поведение для пользователя** — после submit показывать «награда будет после включения в блок»; при запросе баланса с узла, который ещё не синхронизировался, показывать предупреждение или кешировать ответ лидера.  
3. **Retry и таймауты** — воркер и дашборд: повтор при 503/таймауте при «submit к лидеру»; не считать 200 за гарантию попадания в блок, если не реализовано подтверждение включения в цепочку.

---

## Итоговая таблица

| Уровень | Децентрализация |
|---------|------------------|
| Данные (цепочка, балансы) | ✅ Два узла, синхронизация, один консенсус. |
| Создание блоков | ❌ Фактически один узел (node-1) из-за маршрутизации трафика. |
| Код и конфиг узлов | ✅ Узлы равноправны; ротация лидера реализована, но не задействована в submit_work. |

Текущее состояние: **децентрализация данных сохранена, децентрализация создания блоков намеренно ограничена** в пользу простоты и предсказуемости для пользователя.
